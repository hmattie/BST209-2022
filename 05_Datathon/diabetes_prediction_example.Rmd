---
title: "Diabetes Mellitus Prediction"
author: "Tom Pollard, Skyler Shapiro"
date: "2022/06/21"
output: 
  html_document: 
    highlight: haddock
    toc: true
---

## 0. Introduction

This script runs through a simple workflow for training and testing a diabetes mellitus prediction model. Briefly, we will:

1. Load in Required Packages and Gather Data
2. Preprocess the data and remove missing values 
3. Split the data into training and testing sets.
4. Build a random forest classifier using the training set.
5. Evaluate our model on the test set.

You may ask: Why is it important to predict cases of diabetes mellitus in patients?

In the hospital, patient medical records may take days to transfer. Knowledge about chronic conditions  like diabetes can inform clinical decisions about patient care and ultimately improve patient outcomes.

Lets begin!

## 1. Getting Started

### 1.1 Load in Required Packages 

First, we will ensure all the R packages necessary for our project are installed and loaded in.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# options(gargle_oauth_email = TRUE)
options(gargle_verbosity = "info")

```

```{r}

# Checking that all required packages are installed
if(!require("tidyverse")) install.packages("tidyverse")
if(!require("tableone")) install.packages("tableone")
if(!require("caret")) install.packages("caret")
if(!require("dplyr")) install.packages("dplyr")
if(!require("rpart")) install.packages("rpart")
if(!require("rpart.plot")) install.packages("rpart.plot")
if(!require("ROCR")) install.packages("ROCR")
if(!require("knitr")) install.packages("knitr")
if(!require("randomForest")) install.packages("randomForest")

# List of packages to load
packages <- c("tidyverse", "tableone", "caret","dplyr","ROCR", "knitr","randomForest")

# Load packages
lapply(packages, FUN = function(X) {
    do.call("require", list(X)) 
})
```


### 1.2 Load in the Dataset

Access the data from PhysioNet from the [Project Page](https://physionet.org/content/widsdatathon2020/). Download 'training_v2.csv' from the folder.

Now we can load in our data and start to work with it.

```{r}
# Read in data (might take a few seconds)
gossis <- read_csv("~/Downloads/training_v2.csv")
```

```{r}
# Check dimensions of data
# Output format:  rows  by columns
dim(gossis)
``` 

We can see our data has `r dim(gossis)[1]` entries and `r dim(gossis)[2]` Variables. Let's see what our data actually looks like!

```{r}
head(gossis[,1:7]) %>% kable(caption = "Gossis (1st seven variables)")
```

Next, we will select the variables we want to examine, some of which may end up in our final model. Here, we choose to include the following: `bmi`, `age`, `ethnicity`, `gender`, `icu_type`, and our target variable: `diabetes_mellitus`.

```{r}
#  Select subset of variables from original data
reduced_data <- gossis %>% select(bmi, 
                                  age,
                                  gender,
                                  ethnicity,
                                  icu_type,
                                  glucose_apache,
                                  diabetes_mellitus)

head(reduced_data) %>% kable()
```

## 2. Preprocessing and Missing Data

We have to consider the NA values in our data. In a research project, it is critical to investigate the missing data by searching for patterns and replacing missing values (imputation) to see if this affects conclusions. For the simplicity of this demo, we will remove all rows containing missing values.

However, in your project, feel free to experiment with different imputation techniques!

```{r data_types, echo=TRUE}
# Check to see number of missing values in each column
colSums(is.na(reduced_data))

# Check number of rows before removing NA
nrow(reduced_data)

# Remove rows with NA for age, bmi, ethnicity, gender, or diabetes_mellitus
reduced_data <- drop_na(reduced_data)

# Check number of rows after removing NA
nrow(reduced_data)

# We can see there are no NA values in our data
colSums(is.na(reduced_data))
```

Next we will encode our categorical variables. Encoding is the process of reshaping and binarizing categorical data to better suit machine learning models.

```{r}

# Convert ethnicity and diabetes_mellitus to factor
gossis$ethnicity <- as.factor(gossis$ethnicity)
gossis$diabetes_mellitus <- as.factor(gossis$diabetes_mellitus)

# Encode gender variable: male = 1, non-male = 0
gossis$gender <- ifelse(gossis$gender == "M",1,0)
```


## 3. Create the training and testing sets

We will only include four predictors in our model: `age`, `bmi`, `gender`, and `glucose apache`. Feel free to include more or choose entirely different predictors in your model!

```{r training_test, echo=TRUE}
library(caret)
library(dplyr)

# Set the random number seed for reproducibility
set.seed(1)

# CHOOSE VARIABLE YOU WANT TO PREDICT HERE!
reduced_data$target_variable <- as.factor(reduced_data$diabetes_mellitus)

# Create data partition using the target variable
train_index <- createDataPartition(reduced_data$target_variable, times = 1, p = 0.8, list = FALSE)

# Split data into train and test sets, select columns that will be used in model
train_set <- reduced_data[train_index, ] %>% select(., age, bmi, gender, ethnicity,glucose_apache,icu_type, target_variable)
head(train_set)
test_set <- reduced_data[-train_index, ] %>% select(., age, bmi, gender, ethnicity,glucose_apache ,icu_type, target_variable)
head(test_set)
```

## 4. Calculate summary statistics ("Table 1")

- Most studies include summary statistics as Table 1.
- `library(tableone)` makes it easy to create these summary statistics
- Developed by Dr Kazuki Yoshida, while a Ph.D. student at Harvard University.
- [TableOne documentation](https://cran.r-project.org/web/packages/tableone/tableone.pdf)

```{r tableone, echo=TRUE}
library(tableone)

allvars = c("bmi", "age", "gender", "ethnicity","glucose_apache","icu_type")
catvars = c("ethnicity","gender", "icu_type")

table_gossis <- CreateTableOne(vars = allvars, data = train_set, 
                factorVars = catvars,
                strata="target_variable")

kableone(table_gossis)
```

## 5. Model Construction

### 5.1 Train a random forest classifier

- We will use the `randomForest` library to build a random forest model.
- See the [randomForest documentation on CRAN](https://cran.r-project.org/web/packages/randomForest/randomForest.pdf).

```{r train_model, echo=TRUE}
# Define forest tuning parameters

# 5-fold cross validation
control <- trainControl(method='repeatedcv', 
                        number=5, 
                        repeats=3)

# Number of variables tried at each split
mtry <- sqrt(ncol(train_set))

# Grid search is a linear search through a vector of mtry values
tunegrid <- expand.grid(.mtry=mtry)

# Create classification forest using age, bmi, and gender
forest <- randomForest(target_variable ~ age + bmi + gender + glucose_apache, 
              data = train_set,
              metric = "Accuracy",
              tuneGrid = tunegrid,
              trControl = control)
```

### 5.2 View the variable importance

- Many approaches for computing "importance".
- `rpart` calculates a metric using the [sum of contributions to splits]((https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf)).
- Shapley Additive exPlanations (ShAP) values are a popular alternative.
- Compute the marginal contribution of a feature across a combination of features.

```{r importance, echo=TRUE}
library(knitr)
kable(forest$importance)
```
We can see that `glucose_apache` and `bmi` had the most predictive power.


### 5.3 Predict diabetes mellitus in five unseen patients from test set

#### Select unseen patients from testing set

- Let's look five unseen patients in our test set.
- What outcome do you expect for these patients?

```{r select_unseen, echo=TRUE}
unseen <- head(test_set, 5) %>% 
          select(age, bmi, gender, glucose_apache)

print(unseen)
```

#### Predict the diabetes mellitus in unseen patients

- Is the model correct for this patient?
- `type="prob"` to output probabilities
- `type="class"` to output classes

```{r predict_unseen, echo=TRUE}
# Make predictions on testing set
forest_pred <- predict(forest,
                     newdata = test_set,
                     type="class")
# Combine unseen patients data with corresponding predictions
data.frame(age = unseen$age,bmi = unseen$bmi,gender = unseen$gender, glucose_apache = unseen$glucose_apache, prediction = forest_pred[1:5], truth_value = test_set$target_variable[1:5]) %>% kable()
```

## 6. Model Evaluation

### 6.1 Creating our confusion matrix

- A confusion matrix relates predictions to the ground truth.
- Forms the basis for evaluation metrics.
- Alive is our "0" (-ve). Expired is our "1" (+ve).

```{r create_confusion_matrix, echo=TRUE}
library("caret")

# Note that "forest refers to the model trained previously
forest_preds <- predict(forest, 
                     newdata = test_set,
                     type = "class")

# Ground truth is recorded in the GOSSIS data
confusionMatrix(forest_preds,
                test_set$target_variable,
                positive='1')

```

### 6.2 Evaluation Measures

#### 6.2.1 Accuracy

- How many shots did we take? What proportion hit the target?
- 106 shots. 96 hit the mark. Accuracy = ?
- Accuracy = $\frac{(TP + TN)}{(TP + TN + FP + FN)}$

#### 6.2.2 Sensitivity

- (AKA "Recall" AKA "True Positive Rate")
- Of the patients who do not survive, what proportion did we correctly predict?
- 9 people did not survive. We called 2. Sensitivity = ?
- Recall = $\frac{(TP)}{(TP + FN)}$

#### 6.2.1 Specificity 

- (AKA "True Negative Rate")
- What proportion of survivors did we say would survive?
- 97 survivors. We predicted 5 would die. Specificity = ?
- Specificity = $\frac{(TN)}{(TN + FP)}$

#### 6.2.1 Area under ROC Curve (AUC)

- Developed in the 1940s by radar operators.
- Popular measure of discrimination.
- Plots *1 - specificity* vs. *sensitivity* at varying probability thresholds.
- 0.5 is terrible. 1.0 is the dream.
- AUC of 0.9 tells us that the 90% of time our model will assign a higher risk to a randomly selected patient with an event than to a randomly selected patient without an event

We'll use the [ROCR package](https://cran.r-project.org/web/packages/ROCR/index.html) ("Visualizing the Performance of Scoring Classifiers") to plot the curve.

```{r AUROC, echo=TRUE}
library("ROCR")

# Get the probabilities for our forest predictions
forest_probs = predict(forest, newdata = test_set, type = "prob")
forest_probs = forest_probs[,2]

# Create a "prediction" object using our probabilities and the target classes
forest_roc <- prediction(forest_probs, test_set$target_variable)
forest_perf <- performance(forest_roc, measure = "tpr", x.measure = "fpr")

# Plot the ROC curve
plot(forest_perf, col=rainbow(10))
abline(a = 0, b = 1)

# Calculate the AUC
auc_forest <- performance(forest_roc, measure = "auc")
auc_forest <- auc_forest@y.values[[1]]

print(auc_forest)
```


# Over to you!

In you project, it is highly encouraged to explore the following: 

- Add predictors
- Perform imputation on missing values
- Train a more robust model
- Change the prediction target (e.g. LOS)
- Evaluate model calibration
- Evaluate performance in patient subgroups


